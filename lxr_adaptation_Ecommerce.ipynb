{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9Y1Pb5VtidN"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UXsj1XA1s80W",
        "outputId": "07e9e47c-0bbe-4b9c-d10f-d8a8bd1a37f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-3.6.1-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.13.2-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.8.2-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (24.1)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.32)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.66.5)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0.2)\n",
            "Collecting Mako (from alembic>=1.5.0->optuna)\n",
            "  Downloading Mako-1.3.5-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.12.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna) (3.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna) (2.1.5)\n",
            "Downloading optuna-3.6.1-py3-none-any.whl (380 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m380.1/380.1 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.13.2-py3-none-any.whl (232 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.0/233.0 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.8.2-py3-none-any.whl (11 kB)\n",
            "Downloading Mako-1.3.5-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: Mako, colorlog, alembic, optuna\n",
            "Successfully installed Mako-1.3.5 alembic-1.13.2 colorlog-6.8.2 optuna-3.6.1\n"
          ]
        }
      ],
      "source": [
        "! pip install optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "fYLvhbMTstZV"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
        "export_dir = os.getcwd()\n",
        "from pathlib import Path\n",
        "import pickle\n",
        "from collections import defaultdict\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import copy\n",
        "import optuna\n",
        "import logging\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import importlib\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from torch.utils.data import DataLoader, TensorDataset\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MLuRff8VtcWr",
        "outputId": "d0688866-d4d3-464e-b6db-e44e9f366c4e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B0oJxHgCs7Q4",
        "outputId": "cf1c1cc8-a540-498e-9eed-01d72f691853"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "np.random.seed(42)"
      ],
      "metadata": {
        "id": "DMPr7L7O06Py"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# help functions"
      ],
      "metadata": {
        "id": "65D2X6fkZoSe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# a function that wraps the different recommenders types\n",
        "# returns user's scores with respect to a certain item or for all items\n",
        "def recommender_run(user_tensor, recommender, item_tensor = None, item_id= None, wanted_output = 'single', **kw):\n",
        "    output_type=kw['output_type']\n",
        "    if output_type == 'single':\n",
        "        if wanted_output == 'single':\n",
        "            return recommender(user_tensor, item_tensor)\n",
        "        else:\n",
        "            return recommender(user_tensor, item_tensor).squeeze()\n",
        "    else:\n",
        "        if wanted_output == 'single':\n",
        "            return recommender(user_tensor).squeeze()[item_id]\n",
        "        else:\n",
        "            return recommender(user_tensor).squeeze()"
      ],
      "metadata": {
        "id": "Kn0c61QjNUvL"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_user_recommended_item(user_tensor, recommender, **kw):\n",
        "    all_items_tensor = kw['all_items_tensor']\n",
        "    num_items = kw['num_items']\n",
        "    user_res = recommender_run(user_tensor, recommender, all_items_tensor, None, 'vector', **kw)[:num_items]\n",
        "    user_tensor = user_tensor[:num_items]\n",
        "    user_catalog = torch.ones_like(user_tensor) - user_tensor\n",
        "    user_recommendations = torch.mul(user_res, user_catalog)\n",
        "    return torch.argmax(user_recommendations)"
      ],
      "metadata": {
        "id": "DgAPcjLfNDbG"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_index_in_the_list(user_tensor, item_tensor, item_id, model):\n",
        "    scores = model(user_tensor.unsqueeze(0), item_tensor.unsqueeze(0)).cpu().detach().numpy()\n",
        "    sorted_indices = np.argsort(-scores)  # Sort in descending order\n",
        "    return np.where(sorted_indices == item_id)[0][0]"
      ],
      "metadata": {
        "id": "mH00XHlOZHFE"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def recommender_evaluations(model):\n",
        "    counter_10 = 0\n",
        "    counter_50 = 0\n",
        "    counter_100 = 0\n",
        "    RR = 0\n",
        "    PR = 0\n",
        "    temp_test_array = np.array(test_df)\n",
        "    n = temp_test_array.shape[0]\n",
        "    num_items = len(item_encoder.classes_)\n",
        "\n",
        "    for i in range(n):\n",
        "        item_id = temp_test_array[i][-2]\n",
        "        item_tensor = item_matrix[item_id]\n",
        "        user_tensor = torch.Tensor(temp_test_array[i][:-2]).to(device)\n",
        "        user_tensor[item_id] = 0\n",
        "\n",
        "        index = get_index_in_the_list(user_tensor, item_tensor, item_id, model) + 1\n",
        "        if index <= 10:\n",
        "            counter_10 += 1\n",
        "        if index <= 50:\n",
        "            counter_50 += 1\n",
        "        if index <= 100:\n",
        "            counter_100 += 1\n",
        "        RR += np.reciprocal(index)\n",
        "        PR += index / num_items\n",
        "\n",
        "    return counter_10 / n, counter_50 / n, counter_100 / n, RR / n, PR * 100 / n"
      ],
      "metadata": {
        "id": "aS0FDcXhZEkL"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#LXR based similarity\n",
        "def find_LXR_mask(user_tensor, item_id, item_tensor, explainer):\n",
        "    expl_scores = explainer(user_tensor, item_tensor)\n",
        "    x_masked = user_tensor*expl_scores\n",
        "    item_sim_dict = {i: x_masked[i].item() for i in range(len(x_masked))}\n",
        "\n",
        "    return item_sim_dict"
      ],
      "metadata": {
        "id": "0iPjpvSbbbaW"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_pos_neg_k(user_tensor, item_id, items_tensor, num_of_bins, explainer, k=20):\n",
        "    # Initializing masks and computing the user history size\n",
        "    POS_masked = user_tensor.clone()\n",
        "    NEG_masked = user_tensor.clone()\n",
        "    user_hist_size = int(torch.sum(user_tensor))  # The number of positive interactions\n",
        "\n",
        "    # Creating bins for evaluation\n",
        "    bins = [0] + [len(x) for x in np.array_split(np.arange(user_hist_size), num_of_bins, axis=0)]\n",
        "\n",
        "    POS_at_20 = [0] * (num_of_bins + 1)\n",
        "    NEG_at_20 = [0] * (num_of_bins + 1)\n",
        "    total_items = 0\n",
        "\n",
        "    # Use the explainer to find LXR scores\n",
        "    sim_items = find_LXR_mask(user_tensor, item_id, items_tensor, explainer)\n",
        "\n",
        "    # Sort items based on LXR scores (descending for POS, ascending for NEG)\n",
        "    POS_sim_items = list(sorted(sim_items.items(), key=lambda item: item[1], reverse=True))[:user_hist_size]\n",
        "    NEG_sim_items = list(sorted(sim_items.items(), key=lambda item: item[1], reverse=False))[:user_hist_size]\n",
        "\n",
        "    for i in range(len(bins)):\n",
        "        total_items += bins[i]\n",
        "\n",
        "        # Mask positive items\n",
        "        POS_masked = torch.zeros_like(user_tensor, dtype=torch.float32, device=user_tensor.device)\n",
        "        for j in POS_sim_items[:total_items]:\n",
        "            POS_masked[j[0]] = 1\n",
        "        POS_masked = user_tensor - POS_masked  # Remove masked items from user history\n",
        "\n",
        "        # Mask negative items\n",
        "        NEG_masked = torch.zeros_like(user_tensor, dtype=torch.float32, device=user_tensor.device)\n",
        "        for j in NEG_sim_items[:total_items]:\n",
        "            NEG_masked[j[0]] = 1\n",
        "        NEG_masked = user_tensor - NEG_masked  # Remove masked items from user history\n",
        "\n",
        "        # Calculate POS and NEG indices (e.g., ranking of the target item after masking)\n",
        "        POS_index = get_index_in_the_list(POS_masked, user_tensor, item_id, recommender, **kw_dict) + 1\n",
        "        NEG_index = get_index_in_the_list(NEG_masked, user_tensor, item_id, recommender, **kw_dict) + 1\n",
        "\n",
        "        POS_at_20[i] = 1 if POS_index <= 20 else 0\n",
        "        NEG_at_20[i] = 1 if NEG_index <= 20 else 0\n",
        "\n",
        "    res = [np.array(POS_at_20), np.array(NEG_at_20)]\n",
        "    return res\n"
      ],
      "metadata": {
        "id": "StOT93gDbDtj"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_hr_at_k(model, user_tensor, pos_item_tensor, k):\n",
        "    model.eval()\n",
        "    hr_at_k = 0\n",
        "\n",
        "    # Iterate over each user in the batch\n",
        "    for i in range(user_tensor.size(0)):\n",
        "        user_id = user_tensor[i].unsqueeze(0)  # Select the user\n",
        "        pos_item_id = pos_item_tensor[i].unsqueeze(0)  # Select the positive item for the user\n",
        "\n",
        "        # Generate scores for all items for this user\n",
        "        all_item_ids = torch.arange(len(item_encoder.classes_)).to(device)\n",
        "        scores = model(user_id, all_item_ids)\n",
        "\n",
        "        # Adjust k to be at most the number of available items\n",
        "        k_adjusted = min(k, scores.size(0))\n",
        "\n",
        "        # Get the top-k items\n",
        "        _, top_k_indices = torch.topk(scores, k_adjusted)\n",
        "\n",
        "        # Check if the positive item is in the top-k predictions\n",
        "        if pos_item_id in top_k_indices:\n",
        "            hr_at_k += 1\n",
        "\n",
        "    # Compute HR@k\n",
        "    hr_at_k /= user_tensor.size(0)\n",
        "    return hr_at_k\n"
      ],
      "metadata": {
        "id": "SEY57C23ZqC7"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate_pos_at_20(explainer, test_data, k=20):\n",
        "    pos_at_20 = 0\n",
        "    for i, sample in enumerate(test_data):\n",
        "        user_tensor = torch.Tensor(sample[:-1]).to(device)\n",
        "        user_id = int(sample[-1])\n",
        "        top1_item_test = top1_test[user_id]\n",
        "        item_vector = items_array[top1_item_test]\n",
        "        items_tensor = torch.Tensor(item_vector).to(device)\n",
        "\n",
        "        expl_scores = explainer(user_tensor.unsqueeze(0), items_tensor.unsqueeze(0))\n",
        "        # Calculate POS@20 or similar metric\n",
        "        # Placeholder logic\n",
        "        if expl_scores[0][top1_item_test] > 0.5:\n",
        "            pos_at_20 += 1\n",
        "\n",
        "    return pos_at_20 / len(test_data)"
      ],
      "metadata": {
        "id": "WLbu_YTLZqK7"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOjtemFUxfqB"
      },
      "source": [
        "# data import and process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "56sEExLFvJlc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98
        },
        "outputId": "62e5cb70-0c83-4aee-f90b-ccfc486ca029"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4201696 476001\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          user_id     item_id  rating   timestamp\n",
              "0  A2CX7LUOHB2NDG  0321732944     5.0  1341100800"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b99837a3-2146-4102-828f-cbde6527c0d0\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_id</th>\n",
              "      <th>item_id</th>\n",
              "      <th>rating</th>\n",
              "      <th>timestamp</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>A2CX7LUOHB2NDG</td>\n",
              "      <td>0321732944</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1341100800</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b99837a3-2146-4102-828f-cbde6527c0d0')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-b99837a3-2146-4102-828f-cbde6527c0d0 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-b99837a3-2146-4102-828f-cbde6527c0d0');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "ratings_data"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# ratings_data = pd.read_csv('/content/drive/MyDrive/School/DL_methods_apps/project/ratings_Electronics.csv')\n",
        "# ratings_data.columns = ['user_id', 'item_id', 'rating', 'timestamp']\n",
        "# print(ratings_data.user_id.nunique(), ratings_data.item_id.nunique())\n",
        "# ratings_data.head(1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# #delete users with less than 100 rows\n",
        "# interaction_counts = ratings_data.groupby('user_id').size()\n",
        "# active_users = interaction_counts[interaction_counts >= 100].index\n",
        "# filtered_ratings_data = ratings_data[ratings_data['user_id'].isin(active_users)]\n",
        "\n",
        "# print(filtered_ratings_data.user_id.nunique(), filtered_ratings_data.item_id.nunique())\n",
        "# filtered_ratings_data.head(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98
        },
        "id": "T9plsmQMUlbE",
        "outputId": "013ad818-e732-4e05-b149-0355346155fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "289 22635\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           user_id     item_id  rating   timestamp\n",
              "117  AT09WGFUM934H  0594481813     3.0  1377907200"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-518c7aea-1e54-47c9-9f24-687eaa7cf878\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_id</th>\n",
              "      <th>item_id</th>\n",
              "      <th>rating</th>\n",
              "      <th>timestamp</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>117</th>\n",
              "      <td>AT09WGFUM934H</td>\n",
              "      <td>0594481813</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1377907200</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-518c7aea-1e54-47c9-9f24-687eaa7cf878')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-518c7aea-1e54-47c9-9f24-687eaa7cf878 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-518c7aea-1e54-47c9-9f24-687eaa7cf878');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "filtered_ratings_data",
              "summary": "{\n  \"name\": \"filtered_ratings_data\",\n  \"rows\": 44209,\n  \"fields\": [\n    {\n      \"column\": \"user_id\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 289,\n        \"samples\": [\n          \"A3J8A5L5AF5TX9\",\n          \"A19X4BF861LQST\",\n          \"AVFJ327UXPXLF\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"item_id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 22635,\n        \"samples\": [\n          \"B00004SYB7\",\n          \"B00006HWZ5\",\n          \"B003EEMFTS\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"rating\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.0236271862987414,\n        \"min\": 1.0,\n        \"max\": 5.0,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          4.0,\n          2.0,\n          5.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"timestamp\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 82472125,\n        \"min\": 944006400,\n        \"max\": 1406073600,\n        \"num_unique_values\": 4078,\n        \"samples\": [\n          1242604800,\n          1184803200,\n          1082419200\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # create a list of all unique users and items\n",
        "# all_users = filtered_ratings_data['user_id'].unique()\n",
        "# all_items = filtered_ratings_data['item_id'].unique()\n",
        "\n",
        "# # create a DataFrame with all possible user-item combinations\n",
        "# user_item_matrix = pd.DataFrame(index=all_users, columns=all_items)\n",
        "# user_item_matrix = user_item_matrix.fillna(0)  # Initialize all with 0s\n",
        "\n",
        "# # Fill the matrix with 1s where a user has rated an item\n",
        "# for index, row in filtered_ratings_data.iterrows():\n",
        "#     user_item_matrix.at[row['user_id'], row['item_id']] = 1\n",
        "\n",
        "# # Reset the index and convert the DataFrame back into a long-form DataFrame\n",
        "# user_item_df = user_item_matrix.reset_index().melt(id_vars='index', value_name='rating')\n",
        "# user_item_df.columns = ['user_id', 'item_id', 'rating']\n",
        "\n",
        "# # Now `user_item_df` contains all combinations of users and items with a 'rating' of 1 or 0.\n",
        "# print(user_item_df.user_id.nunique(), user_item_df.item_id.nunique(), user_item_df.shape)\n",
        "# user_item_df.head(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98
        },
        "id": "diVoLIhKfDm9",
        "outputId": "a2ca0f2a-e2f3-48df-af89-4aefa98efda2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "289 22635 (6541515, 3)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         user_id     item_id  rating\n",
              "0  AT09WGFUM934H  0594481813       1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c291b6f9-da58-470e-8bf1-01845bb13bd3\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_id</th>\n",
              "      <th>item_id</th>\n",
              "      <th>rating</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>AT09WGFUM934H</td>\n",
              "      <td>0594481813</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c291b6f9-da58-470e-8bf1-01845bb13bd3')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-c291b6f9-da58-470e-8bf1-01845bb13bd3 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-c291b6f9-da58-470e-8bf1-01845bb13bd3');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "user_item_df"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # filtered_user_item_df = pd.read_csv('/content/drive/MyDrive/School/DL_methods_apps/project/user_item_df_processed.csv')\n",
        "# filtered_user_item_df = pd.read_csv('/content/drive/MyDrive/Tamar/project/user_item_df_processed.csv')\n",
        "# print(filtered_user_item_df.user_id.nunique(), filtered_user_item_df.item_id.nunique(), filtered_user_item_df.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FySEFuwxS92S",
        "outputId": "9457a351-0498-4198-a7c4-548925085ea9"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "289 22635 (6541515, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# #further filter the data\n",
        "# filtered_user_item_df.columns = ['user_id','item_id','interaction']\n",
        "\n",
        "# import random\n",
        "# sampled_items = random.sample(list(filtered_user_item_df['item_id'].unique()), 1500)\n",
        "# filtered_user_item_df = filtered_user_item_df[\n",
        "#     filtered_user_item_df['item_id'].isin(sampled_items)\n",
        "# ]"
      ],
      "metadata": {
        "id": "ZOu3of8eeguH"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_user_item_df = pd.read_csv('/content/drive/MyDrive/Tamar/project/user_item_df_final.csv')"
      ],
      "metadata": {
        "id": "7taTQvX3xlx2"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(filtered_user_item_df.user_id.nunique(), filtered_user_item_df.item_id.nunique(), filtered_user_item_df.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IHILNM8rfncV",
        "outputId": "a6fff80e-1d0a-4bc5-ca11-1d91adba74c3"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "289 1500 (433500, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "OEJVFbjVuuoi"
      },
      "outputs": [],
      "source": [
        "n_users = 289\n",
        "n_items = 1500"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "Zd1j1ywzxkWs"
      },
      "outputs": [],
      "source": [
        "# Encode user_id and item_id as integers\n",
        "user_encoder = LabelEncoder()\n",
        "item_encoder = LabelEncoder()\n",
        "\n",
        "filtered_user_item_df['user_id'] = user_encoder.fit_transform(filtered_user_item_df['user_id'])\n",
        "filtered_user_item_df['item_id'] = item_encoder.fit_transform(filtered_user_item_df['item_id'])\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "train_df, test_df = train_test_split(filtered_user_item_df, test_size=0.2, random_state=42)\n",
        "\n",
        "# Prepare the training and testing data\n",
        "X_train = train_df[['user_id', 'item_id']].values\n",
        "y_train = train_df['interaction'].values\n",
        "X_test = test_df[['user_id', 'item_id']].values\n",
        "y_test = test_df['interaction'].values\n",
        "\n",
        "num_items = len(item_encoder.classes_)\n",
        "item_matrix = np.eye(num_items)  # Identity matrix representing one-hot encoded items"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oD_EwoDKtlbT"
      },
      "source": [
        "# Recommender's Handle (same as paper)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLj_J5FBxEtb"
      },
      "source": [
        "## architecture"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# class MLP(nn.Module):\n",
        "#     def __init__(self, hidden_size, device, **kw):\n",
        "#         super(MLP, self).__init__()\n",
        "#         user_size = 289  # Adjusted to the number of users\n",
        "#         item_size = 1500  # Adjusted to the number of items\n",
        "#         self.device = device\n",
        "#         self.hidden_size = hidden_size\n",
        "#         self.users_fc = nn.Linear(user_size, hidden_size, bias=True).to(self.device)\n",
        "#         self.items_fc = nn.Linear(item_size, hidden_size, bias=True).to(self.device)\n",
        "#         self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "#     def forward(self, user_tensor, item_tensor):\n",
        "#         # Ensure that tensors are correctly sized and converted to float\n",
        "#         user_tensor = user_tensor.float().to(self.device)  # [batch_size, num_users]\n",
        "#         item_tensor = item_tensor.float().to(self.device)  # [batch_size, num_items]\n",
        "\n",
        "#         user_vec = self.users_fc(user_tensor)\n",
        "#         item_vec = self.items_fc(item_tensor)\n",
        "\n",
        "#         # Perform matrix multiplication of the resulting vectors\n",
        "#         output = torch.matmul(user_vec, item_vec.T).to(self.device)\n",
        "#         return self.sigmoid(output).to(self.device)"
      ],
      "metadata": {
        "id": "wcm-_KB8THyb"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, user_size, item_size, hidden_size, device):\n",
        "        super(MLP, self).__init__()\n",
        "        self.device = device\n",
        "        self.user_size = user_size\n",
        "        self.item_size = item_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # Linear layers\n",
        "        self.users_fc = nn.Linear(self.user_size, self.hidden_size, bias=True).to(self.device)\n",
        "        self.items_fc = nn.Linear(self.item_size, self.hidden_size, bias=True).to(self.device)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, user_tensor, item_tensor):\n",
        "        # Ensure the tensors are on the correct device and have the right dtype\n",
        "        user_tensor = user_tensor.to(self.device).float()\n",
        "        item_tensor = item_tensor.to(self.device).float()\n",
        "        # Forward pass through the linear layers\n",
        "        user_vec = self.users_fc(user_tensor)  # [batch_size, hidden_size]\n",
        "        item_vec = self.items_fc(item_tensor)  # [batch_size, hidden_size]\n",
        "        # Multiply the user and item vectors\n",
        "        output = torch.sum(user_vec * item_vec, dim=1)  # [batch_size]\n",
        "        return self.sigmoid(output).to(self.device)\n"
      ],
      "metadata": {
        "id": "F0ScXKw3B1Al"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFylJPXHzVon"
      },
      "source": [
        "## HP tuning (optuna)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "G61dmB_RzX1X"
      },
      "outputs": [],
      "source": [
        "# Global dictionaries for storing results\n",
        "train_losses_dict = {}\n",
        "test_losses_dict = {}\n",
        "HR10_dict = {}\n",
        "\n",
        "def MLP_objective(trial):\n",
        "    # Hyperparameters to tune\n",
        "    lr = trial.suggest_float('learning_rate', 0.001, 0.01)\n",
        "    batch_size = trial.suggest_categorical('batch_size', [32, 64])\n",
        "    hidden_dim = trial.suggest_categorical('hidden_dim', [32, 64, 128])\n",
        "    beta = trial.suggest_float('beta', 0, 4)\n",
        "    epochs = 10\n",
        "\n",
        "    num_users = len(user_encoder.classes_)\n",
        "    num_items = len(item_encoder.classes_)\n",
        "\n",
        "    # Initialize model, optimizer, and loss function\n",
        "    model = MLP(hidden_dim, num_users=num_users, num_items=num_items, device=device).to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.BCELoss()\n",
        "\n",
        "    train_losses = []\n",
        "    hr10 = []\n",
        "\n",
        "    num_training = train_df.shape[0]\n",
        "    num_batches = int(np.ceil(num_training / batch_size))\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Shuffle training data\n",
        "        perm = np.random.permutation(num_training)\n",
        "        epoch_loss = 0\n",
        "\n",
        "        for b in range(num_batches):\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Create batch\n",
        "            batch_idx = perm[b * batch_size:] if (b + 1) * batch_size >= num_training else perm[b * batch_size: (b + 1) * batch_size]\n",
        "            user_ids = torch.LongTensor(train_df.iloc[batch_idx]['user_id'].values).to(device)\n",
        "            pos_item_ids = torch.LongTensor(train_df.iloc[batch_idx]['item_id'].values).to(device)\n",
        "\n",
        "            # One-hot encode the user and item IDs on the same device\n",
        "            user_tensor = torch.eye(num_users, device=device)[user_ids]\n",
        "            pos_item_tensor = torch.eye(num_items, device=device)[pos_item_ids]\n",
        "\n",
        "            # Positive predictions\n",
        "            pos_output = torch.diagonal(model(user_tensor, pos_item_tensor))\n",
        "\n",
        "            # Calculate loss\n",
        "            pos_loss = criterion(pos_output, torch.ones_like(pos_output))\n",
        "            neg_loss = criterion(pos_output, torch.zeros_like(pos_output))\n",
        "\n",
        "            batch_loss = pos_loss + beta * neg_loss\n",
        "            batch_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += batch_loss.item()\n",
        "\n",
        "        train_losses.append(epoch_loss / num_batches)\n",
        "        print(f'Epoch {epoch + 1}/{epochs}, Loss: {epoch_loss / num_batches}')\n",
        "\n",
        "        # Evaluation phase using HR@10\n",
        "        hr_at_10 = compute_hr_at_k(model, user_tensor, pos_item_tensor, k=10)\n",
        "        hr10.append(hr_at_10)\n",
        "        print(f'Epoch {epoch + 1}/{epochs}, HR@10: {hr_at_10}')\n",
        "\n",
        "        # Early stopping (if necessary)\n",
        "        if epoch > 5 and hr10[-1] > max(hr10[:-1]):\n",
        "            print(\"Early stopping...\")\n",
        "            break\n",
        "\n",
        "    # Save the best model and return the best HR@10\n",
        "    torch.save(model.state_dict(), f'/content/drive/MyDrive/Tamar/project/trained_recommenders/trained_best_rec_trial_{trial.number}_epoch_{epoch + 1}_{max(hr10)}.pth')\n",
        "    return max(hr10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QsXE1Uzzzz4M",
        "outputId": "aacac5b3-d759-4114-c253-811ec41cf327",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-08-24 14:18:58,288] A new study created in memory with name: no-name-9961e63a-2394-4989-999d-77c2f800c5c8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 1.952631319915485\n",
            "Epoch 1/10, HR@10: 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-37-0346f2f950ed>:20: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3675.)\n",
            "  output = torch.matmul(user_vec, item_vec.T).to(self.device)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/10, Loss: 1.949782993800527\n",
            "Epoch 2/10, HR@10: 1.0\n",
            "Epoch 3/10, Loss: 1.951545982425515\n",
            "Epoch 3/10, HR@10: 1.0\n",
            "Epoch 4/10, Loss: 1.950940334845302\n",
            "Epoch 4/10, HR@10: 1.0\n",
            "Epoch 5/10, Loss: 1.950622779644598\n",
            "Epoch 5/10, HR@10: 1.0\n",
            "Epoch 6/10, Loss: 1.9509793326167917\n",
            "Epoch 6/10, HR@10: 1.0\n",
            "Epoch 7/10, Loss: 1.9509649297741862\n",
            "Epoch 7/10, HR@10: 1.0\n",
            "Epoch 8/10, Loss: 1.9507046168108755\n",
            "Epoch 8/10, HR@10: 1.0\n",
            "Epoch 9/10, Loss: 1.9509396847048068\n",
            "Epoch 9/10, HR@10: 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-08-24 14:23:02,014] Trial 0 finished with value: 1.0 and parameters: {'learning_rate': 0.005393857000190963, 'batch_size': 32, 'hidden_dim': 128, 'beta': 2.050872265667402}. Best is trial 0 with value: 1.0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/10, Loss: 1.9508680800248566\n",
            "Epoch 10/10, HR@10: 1.0\n",
            "Epoch 1/10, Loss: 1.5953341441788913\n",
            "Epoch 1/10, HR@10: 1.0\n",
            "Epoch 2/10, Loss: 1.6077741323790662\n",
            "Epoch 2/10, HR@10: 1.0\n",
            "Epoch 3/10, Loss: 1.5958032478901663\n",
            "Epoch 3/10, HR@10: 1.0\n",
            "Epoch 4/10, Loss: 1.6084311600430115\n",
            "Epoch 4/10, HR@10: 1.0\n",
            "Epoch 5/10, Loss: 1.5955437116241034\n",
            "Epoch 5/10, HR@10: 1.0\n",
            "Epoch 6/10, Loss: 1.6064329977558205\n",
            "Epoch 6/10, HR@10: 1.0\n",
            "Epoch 7/10, Loss: 1.5967309030577537\n",
            "Epoch 7/10, HR@10: 1.0\n",
            "Epoch 8/10, Loss: 1.6082174141131154\n",
            "Epoch 8/10, HR@10: 1.0\n",
            "Epoch 9/10, Loss: 1.595596050221798\n",
            "Epoch 9/10, HR@10: 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-08-24 14:25:08,796] Trial 1 finished with value: 1.0 and parameters: {'learning_rate': 0.00603354974575049, 'batch_size': 64, 'hidden_dim': 128, 'beta': 1.3258095818037274}. Best is trial 0 with value: 1.0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/10, Loss: 1.6074561133202558\n",
            "Epoch 10/10, HR@10: 1.0\n",
            "Epoch 1/10, Loss: 1.551689158442452\n",
            "Epoch 1/10, HR@10: 1.0\n",
            "Epoch 2/10, Loss: 1.5516628171271736\n",
            "Epoch 2/10, HR@10: 1.0\n",
            "Epoch 3/10, Loss: 1.5517548518463307\n",
            "Epoch 3/10, HR@10: 1.0\n",
            "Epoch 4/10, Loss: 1.5524507851085005\n",
            "Epoch 4/10, HR@10: 1.0\n",
            "Epoch 5/10, Loss: 1.5515743584887174\n",
            "Epoch 5/10, HR@10: 1.0\n",
            "Epoch 6/10, Loss: 1.5516301868894176\n",
            "Epoch 6/10, HR@10: 1.0\n",
            "Epoch 7/10, Loss: 1.5524445159405795\n",
            "Epoch 7/10, HR@10: 1.0\n",
            "Epoch 8/10, Loss: 1.551605068277798\n",
            "Epoch 8/10, HR@10: 1.0\n",
            "Epoch 9/10, Loss: 1.5516029742068087\n",
            "Epoch 9/10, HR@10: 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-08-24 14:29:11,771] Trial 2 finished with value: 1.0 and parameters: {'learning_rate': 0.006494750497228074, 'batch_size': 32, 'hidden_dim': 32, 'beta': 1.2577495608763574}. Best is trial 0 with value: 1.0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/10, Loss: 1.5524387131935777\n",
            "Epoch 10/10, HR@10: 1.0\n",
            "Epoch 1/10, Loss: 0.9055878495043727\n",
            "Epoch 1/10, HR@10: 1.0\n",
            "Epoch 2/10, Loss: 0.905205545917739\n",
            "Epoch 2/10, HR@10: 1.0\n",
            "Epoch 3/10, Loss: 0.9051868816785078\n",
            "Epoch 3/10, HR@10: 1.0\n",
            "Epoch 4/10, Loss: 0.9050768417039686\n",
            "Epoch 4/10, HR@10: 1.0\n",
            "Epoch 5/10, Loss: 0.9051711126367288\n",
            "Epoch 5/10, HR@10: 1.0\n",
            "Epoch 6/10, Loss: 0.9051935885924434\n",
            "Epoch 6/10, HR@10: 1.0\n",
            "Epoch 7/10, Loss: 0.905189059849352\n",
            "Epoch 7/10, HR@10: 1.0\n",
            "Epoch 8/10, Loss: 0.9051912030097544\n",
            "Epoch 8/10, HR@10: 1.0\n",
            "Epoch 9/10, Loss: 0.9051947735468605\n",
            "Epoch 9/10, HR@10: 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-08-24 14:31:16,822] Trial 3 finished with value: 1.0 and parameters: {'learning_rate': 0.004775710213802556, 'batch_size': 64, 'hidden_dim': 32, 'beta': 0.4559041576995275}. Best is trial 0 with value: 1.0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/10, Loss: 0.9051837465225891\n",
            "Epoch 10/10, HR@10: 1.0\n",
            "Epoch 1/10, Loss: 2.4716682527355216\n",
            "Epoch 1/10, HR@10: 1.0\n",
            "Epoch 2/10, Loss: 2.4658890189797176\n",
            "Epoch 2/10, HR@10: 1.0\n",
            "Epoch 3/10, Loss: 2.465682812470927\n",
            "Epoch 3/10, HR@10: 1.0\n",
            "Epoch 4/10, Loss: 2.4655494230175\n",
            "Epoch 4/10, HR@10: 1.0\n",
            "Epoch 5/10, Loss: 2.465538281711579\n",
            "Epoch 5/10, HR@10: 1.0\n",
            "Epoch 6/10, Loss: 2.465533334757046\n",
            "Epoch 6/10, HR@10: 1.0\n",
            "Epoch 7/10, Loss: 2.465524849405058\n",
            "Epoch 7/10, HR@10: 1.0\n",
            "Epoch 8/10, Loss: 2.465494850109063\n",
            "Epoch 8/10, HR@10: 1.0\n",
            "Epoch 9/10, Loss: 2.465473462259842\n",
            "Epoch 9/10, HR@10: 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-08-24 14:33:21,122] Trial 4 finished with value: 1.0 and parameters: {'learning_rate': 0.002132070118787784, 'batch_size': 64, 'hidden_dim': 32, 'beta': 3.8388398498063245}. Best is trial 0 with value: 1.0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/10, Loss: 2.4654711982732684\n",
            "Epoch 10/10, HR@10: 1.0\n",
            "Epoch 1/10, Loss: 2.05895060136939\n",
            "Epoch 1/10, HR@10: 1.0\n",
            "Epoch 2/10, Loss: 2.060138047874403\n",
            "Epoch 2/10, HR@10: 1.0\n",
            "Epoch 3/10, Loss: 2.060336882105875\n",
            "Epoch 3/10, HR@10: 1.0\n",
            "Epoch 4/10, Loss: 2.0606485531765237\n",
            "Epoch 4/10, HR@10: 1.0\n",
            "Epoch 5/10, Loss: 2.0603370131502996\n",
            "Epoch 5/10, HR@10: 1.0\n",
            "Epoch 6/10, Loss: 2.0600648276105007\n",
            "Epoch 6/10, HR@10: 1.0\n",
            "Epoch 7/10, Loss: 2.060023875270829\n",
            "Epoch 7/10, HR@10: 1.0\n",
            "Epoch 8/10, Loss: 2.0592277048698406\n",
            "Epoch 8/10, HR@10: 1.0\n",
            "Epoch 9/10, Loss: 2.0580463454740516\n",
            "Epoch 9/10, HR@10: 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-08-24 14:37:22,615] Trial 5 finished with value: 1.0 and parameters: {'learning_rate': 0.008910981756659109, 'batch_size': 32, 'hidden_dim': 64, 'beta': 2.303301528634996}. Best is trial 0 with value: 1.0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/10, Loss: 2.0567969408922253\n",
            "Epoch 10/10, HR@10: 1.0\n",
            "Epoch 1/10, Loss: 2.2165515364745287\n",
            "Epoch 1/10, HR@10: 1.0\n",
            "Epoch 2/10, Loss: 2.266673566274789\n",
            "Epoch 2/10, HR@10: 1.0\n",
            "Epoch 3/10, Loss: 2.2229163497548896\n",
            "Epoch 3/10, HR@10: 1.0\n",
            "Epoch 4/10, Loss: 2.262539152950351\n",
            "Epoch 4/10, HR@10: 1.0\n",
            "Epoch 5/10, Loss: 2.2248004126051604\n",
            "Epoch 5/10, HR@10: 1.0\n",
            "Epoch 6/10, Loss: 2.262939505349184\n",
            "Epoch 6/10, HR@10: 1.0\n",
            "Epoch 7/10, Loss: 2.2253171479840588\n",
            "Epoch 7/10, HR@10: 1.0\n",
            "Epoch 8/10, Loss: 2.262163670702292\n",
            "Epoch 8/10, HR@10: 1.0\n",
            "Epoch 9/10, Loss: 2.224366919495871\n",
            "Epoch 9/10, HR@10: 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-08-24 14:39:27,496] Trial 6 finished with value: 1.0 and parameters: {'learning_rate': 0.007852979391799377, 'batch_size': 64, 'hidden_dim': 128, 'beta': 2.8129727040187045}. Best is trial 0 with value: 1.0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/10, Loss: 2.2613735494633005\n",
            "Epoch 10/10, HR@10: 1.0\n",
            "Epoch 1/10, Loss: 2.2209893522897888\n",
            "Epoch 1/10, HR@10: 1.0\n",
            "Epoch 2/10, Loss: 2.2204199395599478\n",
            "Epoch 2/10, HR@10: 1.0\n",
            "Epoch 3/10, Loss: 2.220443013497969\n",
            "Epoch 3/10, HR@10: 1.0\n",
            "Epoch 4/10, Loss: 2.2203820354275474\n",
            "Epoch 4/10, HR@10: 1.0\n",
            "Epoch 5/10, Loss: 2.2203400942601275\n",
            "Epoch 5/10, HR@10: 1.0\n",
            "Epoch 6/10, Loss: 2.220296597080403\n",
            "Epoch 6/10, HR@10: 1.0\n",
            "Epoch 7/10, Loss: 2.2202831248907966\n",
            "Epoch 7/10, HR@10: 1.0\n",
            "Epoch 8/10, Loss: 2.2203042123739647\n",
            "Epoch 8/10, HR@10: 1.0\n",
            "Epoch 9/10, Loss: 2.220249520410227\n",
            "Epoch 9/10, HR@10: 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-08-24 14:43:29,390] Trial 7 finished with value: 1.0 and parameters: {'learning_rate': 0.0044661994254919166, 'batch_size': 32, 'hidden_dim': 64, 'beta': 2.8904057892399457}. Best is trial 0 with value: 1.0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/10, Loss: 2.2202850164005867\n",
            "Epoch 10/10, HR@10: 1.0\n",
            "Epoch 1/10, Loss: 2.2922967189043337\n",
            "Epoch 1/10, HR@10: 1.0\n",
            "Epoch 2/10, Loss: 2.28888834338937\n",
            "Epoch 2/10, HR@10: 1.0\n",
            "Epoch 3/10, Loss: 2.2888172078031435\n",
            "Epoch 3/10, HR@10: 1.0\n",
            "Epoch 4/10, Loss: 2.288775116034728\n",
            "Epoch 4/10, HR@10: 1.0\n",
            "Epoch 5/10, Loss: 2.288749756199734\n",
            "Epoch 5/10, HR@10: 1.0\n",
            "Epoch 6/10, Loss: 2.28873552167783\n",
            "Epoch 6/10, HR@10: 1.0\n",
            "Epoch 7/10, Loss: 2.288727140708215\n",
            "Epoch 7/10, HR@10: 1.0\n",
            "Epoch 8/10, Loss: 2.288713628041566\n",
            "Epoch 8/10, HR@10: 1.0\n",
            "Epoch 9/10, Loss: 2.2887129190993676\n",
            "Epoch 9/10, HR@10: 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-08-24 14:47:32,604] Trial 8 finished with value: 1.0 and parameters: {'learning_rate': 0.0010730416752593671, 'batch_size': 32, 'hidden_dim': 32, 'beta': 3.1394425832029214}. Best is trial 0 with value: 1.0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/10, Loss: 2.2887122626453333\n",
            "Epoch 10/10, HR@10: 1.0\n",
            "Epoch 1/10, Loss: 1.4149559637479048\n",
            "Epoch 1/10, HR@10: 1.0\n",
            "Epoch 2/10, Loss: 1.4145129016018108\n",
            "Epoch 2/10, HR@10: 1.0\n",
            "Epoch 3/10, Loss: 1.41486539638753\n",
            "Epoch 3/10, HR@10: 1.0\n",
            "Epoch 4/10, Loss: 1.414898624981565\n",
            "Epoch 4/10, HR@10: 1.0\n",
            "Epoch 5/10, Loss: 1.4145007624268027\n",
            "Epoch 5/10, HR@10: 1.0\n",
            "Epoch 6/10, Loss: 1.4148663919687843\n",
            "Epoch 6/10, HR@10: 1.0\n",
            "Epoch 7/10, Loss: 1.4149215741886998\n",
            "Epoch 7/10, HR@10: 1.0\n",
            "Epoch 8/10, Loss: 1.4144777514041844\n",
            "Epoch 8/10, HR@10: 1.0\n",
            "Epoch 9/10, Loss: 1.4149020793231613\n",
            "Epoch 9/10, HR@10: 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-08-24 14:49:37,445] Trial 9 finished with value: 1.0 and parameters: {'learning_rate': 0.0037553251409142997, 'batch_size': 64, 'hidden_dim': 64, 'beta': 1.0411302058083782}. Best is trial 0 with value: 1.0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/10, Loss: 1.4148879993399652\n",
            "Epoch 10/10, HR@10: 1.0\n",
            "Best trial:\n",
            "  HR@10: 1.0\n",
            "  Best hyperparameters:  {'learning_rate': 0.005393857000190963, 'batch_size': 32, 'hidden_dim': 128, 'beta': 2.050872265667402}\n"
          ]
        }
      ],
      "source": [
        "study = optuna.create_study(direction='maximize')\n",
        "study.optimize(MLP_objective, n_trials=10)\n",
        "\n",
        "# Print the best trial results\n",
        "print(\"Best trial:\")\n",
        "trial = study.best_trial\n",
        "print(f\"  HR@10: {trial.value}\")\n",
        "print(\"  Best hyperparameters: \", trial.params)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "Uj4ZEv2Cz6ut"
      },
      "outputs": [],
      "source": [
        "# Retrieve the best hyperparameters from the Optuna trial\n",
        "best_params = trial.params\n",
        "\n",
        "# Initialize the MLPRecommender model with the best hyperparameters\n",
        "model = MLP(user_size=len(user_encoder.classes_),\n",
        "                       item_size=len(item_encoder.classes_),\n",
        "                       hidden_size=best_params['hidden_dim'], device= device).to(device)\n",
        "\n",
        "# Initialize the optimizer with the best learning rate\n",
        "optimizer = optim.Adam(model.parameters(), lr=best_params['learning_rate'])\n",
        "\n",
        "best_beta = best_params['beta']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.hidden_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C1zU_5O66F0e",
        "outputId": "95689a72-5d1c-4006-90b1-e0fefa4cf0db"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "128"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(best_params) #print so we can create the dict"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "twF2sPAThiFm",
        "outputId": "edce82ec-6a84-4600-afc4-dda646b98784"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'learning_rate': 0.005393857000190963, 'batch_size': 32, 'hidden_dim': 128, 'beta': 2.050872265667402}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73hNJy892jkF"
      },
      "source": [
        "## loading trained rec"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_params = {'learning_rate': 0.005393857000190963, 'batch_size': 32, 'hidden_dim': 128, 'beta': 2.050872265667402}"
      ],
      "metadata": {
        "id": "GgwckgsVtIsE"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6pMCKYah2ZM0"
      },
      "outputs": [],
      "source": [
        "#create model based on chosen hp\n",
        "model = MLP(user_size=len(user_encoder.classes_),\n",
        "                       item_size=len(item_encoder.classes_),\n",
        "                       hidden_size=best_params['hidden_dim'], device= device).to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=best_params['learning_rate'])\n",
        "\n",
        "#load best trained model\n",
        "model.load_state_dict(torch.load(f'/content/drive/MyDrive/Tamar/project/trained_recommenders/trained_best_rec_trial_0_epoch_10_1.0.pth'), )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3vJruvOJ5Qvg"
      },
      "source": [
        "# LXR adaptation to amazon dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3aIVXxmq-NdD"
      },
      "source": [
        "## data handling"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_user_item_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "3lpCviRfB_O2",
        "outputId": "cc2166b4-4a83-4349-faf0-72daee4af5e5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        user_id  item_id  interaction\n",
              "0           274        0            0\n",
              "1            15        0            0\n",
              "2           216        0            0\n",
              "3            19        0            0\n",
              "4           224        0            0\n",
              "...         ...      ...          ...\n",
              "433495       52     1499            0\n",
              "433496      126     1499            0\n",
              "433497      195     1499            0\n",
              "433498      174     1499            0\n",
              "433499      159     1499            0\n",
              "\n",
              "[433500 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5ccf5e67-e29b-4b99-8745-c4d6887e066e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_id</th>\n",
              "      <th>item_id</th>\n",
              "      <th>interaction</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>274</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>15</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>216</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>19</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>224</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>433495</th>\n",
              "      <td>52</td>\n",
              "      <td>1499</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>433496</th>\n",
              "      <td>126</td>\n",
              "      <td>1499</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>433497</th>\n",
              "      <td>195</td>\n",
              "      <td>1499</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>433498</th>\n",
              "      <td>174</td>\n",
              "      <td>1499</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>433499</th>\n",
              "      <td>159</td>\n",
              "      <td>1499</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>433500 rows × 3 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5ccf5e67-e29b-4b99-8745-c4d6887e066e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-5ccf5e67-e29b-4b99-8745-c4d6887e066e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-5ccf5e67-e29b-4b99-8745-c4d6887e066e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-2673548b-ec1e-46c8-a376-8d6f17f88162\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-2673548b-ec1e-46c8-a376-8d6f17f88162')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-2673548b-ec1e-46c8-a376-8d6f17f88162 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_460da601-6919-4237-a4ed-2c7d1f672510\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('filtered_user_item_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_460da601-6919-4237-a4ed-2c7d1f672510 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('filtered_user_item_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "filtered_user_item_df"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load final df\n",
        "filtered_user_item_df = pd.read_csv('/content/drive/MyDrive/Tamar/project/user_item_df_final.csv')\n",
        "filtered_user_item_df = filtered_user_item_df[['user_id','item_id','interaction']].copy()\n",
        "\n",
        "# Ensure all columns are treated as strings for IDs and integers for interactions\n",
        "filtered_user_item_df['user_id'] = filtered_user_item_df['user_id'].astype(str)\n",
        "filtered_user_item_df['item_id'] = filtered_user_item_df['item_id'].astype(str)\n",
        "filtered_user_item_df['interaction'] = filtered_user_item_df['interaction'].astype(int)\n",
        "\n",
        "# Encode user_id and item_id as integers across the entire dataset\n",
        "user_encoder = LabelEncoder()\n",
        "item_encoder = LabelEncoder()\n",
        "\n",
        "# Fit the encoders on the entire dataset before splitting\n",
        "filtered_user_item_df['user_id'] = user_encoder.fit_transform(filtered_user_item_df['user_id'])\n",
        "filtered_user_item_df['item_id'] = item_encoder.fit_transform(filtered_user_item_df['item_id'])\n",
        "\n",
        "# Now split the data into training and testing sets\n",
        "train_df, test_df = train_test_split(filtered_user_item_df, test_size=0.2, random_state=42)\n",
        "\n",
        "# Prepare PyTorch datasets and data loaders\n",
        "X_train = train_df[['user_id', 'item_id']].values\n",
        "y_train = train_df['interaction'].values.astype(float)  # Ensure interaction labels are floats\n",
        "X_test = test_df[['user_id', 'item_id']].values\n",
        "y_test = test_df['interaction'].values.astype(float)  # Ensure interaction labels are floats\n",
        "\n",
        "train_dataset = TensorDataset(torch.tensor(X_train[:, 0], dtype=torch.long),\n",
        "                              torch.tensor(X_train[:, 1], dtype=torch.long),\n",
        "                              torch.tensor(y_train, dtype=torch.float))\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "test_dataset = TensorDataset(torch.tensor(X_test[:, 0], dtype=torch.long),\n",
        "                             torch.tensor(X_test[:, 1], dtype=torch.long),\n",
        "                             torch.tensor(y_test, dtype=torch.float))\n",
        "test_loader = DataLoader(test_dataset, batch_size=512, shuffle=False)\n"
      ],
      "metadata": {
        "id": "4pPm7Q_hFO1f"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(filtered_user_item_df.user_id.nunique(), filtered_user_item_df.item_id.nunique(), filtered_user_item_df.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9410b4d-55be-4292-d884-b1f9fa689915",
        "id": "Z3cIZBXz6dW3"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "289 1500 (433500, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "YLBNa2VI6dXG"
      },
      "outputs": [],
      "source": [
        "n_users = 289\n",
        "n_items = 1500"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_array = train_df.to_numpy()\n",
        "test_array = test_df.to_numpy()"
      ],
      "metadata": {
        "id": "0OTcxLwr_PjA"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tCUojan-RSS"
      },
      "source": [
        "## architecture + loss"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Explainer(nn.Module):\n",
        "    def __init__(self, user_size, item_size, hidden_size):\n",
        "        super(Explainer, self).__init__()\n",
        "        self.user_size = user_size\n",
        "        self.item_size = item_size\n",
        "\n",
        "        # Define the fully connected layers for users and items\n",
        "        self.users_fc = nn.Linear(in_features=self.user_size, out_features=hidden_size)\n",
        "        self.items_fc = nn.Linear(in_features=self.item_size, out_features=hidden_size)\n",
        "\n",
        "        # Define the bottleneck neural network\n",
        "        self.bottleneck = nn.Sequential(\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(in_features=hidden_size * 2, out_features=hidden_size),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(in_features=hidden_size, out_features=1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, user_tensor, item_tensor):\n",
        "        # Ensure that user_tensor and item_tensor have the correct dimensions\n",
        "        if len(user_tensor.shape) == 1:\n",
        "            user_tensor = user_tensor.unsqueeze(0)\n",
        "        if len(item_tensor.shape) == 1:\n",
        "            item_tensor = item_tensor.unsqueeze(0)\n",
        "\n",
        "        if user_tensor.shape[1] != self.user_size or item_tensor.shape[1] != self.item_size:\n",
        "            raise ValueError(f\"Expected user_tensor shape [batch_size, {self.user_size}] and item_tensor shape [batch_size, {self.item_size}], but got {user_tensor.shape} and {item_tensor.shape}.\")\n",
        "\n",
        "        user_output = self.users_fc(user_tensor.float())\n",
        "        item_output = self.items_fc(item_tensor.float())\n",
        "\n",
        "        # Combine the user and item outputs\n",
        "        combined_output = torch.cat((user_output, item_output), dim=-1)\n",
        "\n",
        "        # Pass the combined output through the bottleneck network\n",
        "        expl_scores = self.bottleneck(combined_output)\n",
        "        return expl_scores\n"
      ],
      "metadata": {
        "id": "gM_WFXSZr4jI"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LXR_loss(nn.Module):\n",
        "    def __init__(self, lambda_pos, lambda_neg, alpha, recommender):\n",
        "        super(LXR_loss, self).__init__()\n",
        "        self.lambda_pos = lambda_pos\n",
        "        self.lambda_neg = lambda_neg\n",
        "        self.alpha = alpha\n",
        "        self.recommender = recommender\n",
        "\n",
        "    def forward(self, user_tensors, items_tensors, items_ids, pos_masks):\n",
        "        # print(f\"LXR_loss forward pass:\")\n",
        "        # print(f\"  user_tensors.shape: {user_tensors.shape}\")\n",
        "        # print(f\"  items_tensors.shape: {items_tensors.shape}\")\n",
        "\n",
        "        neg_masks = torch.sub(torch.ones_like(pos_masks), pos_masks)\n",
        "        x_masked_pos = user_tensors * pos_masks\n",
        "        x_masked_neg = user_tensors * neg_masks\n",
        "\n",
        "        # print(f\"  x_masked_pos.shape: {x_masked_pos.shape}\")\n",
        "        # print(f\"  x_masked_neg.shape: {x_masked_neg.shape}\")\n",
        "\n",
        "        x_masked_res_pos = torch.diag(self.recommender(x_masked_pos.float(), items_tensors.float()))\n",
        "        x_masked_res_neg = torch.diag(self.recommender(x_masked_neg.float(), items_tensors.float()))\n",
        "\n",
        "        # print(f\"  x_masked_res_pos.shape: {x_masked_res_pos.shape}\")\n",
        "        # print(f\"  x_masked_res_neg.shape: {x_masked_res_neg.shape}\")\n",
        "\n",
        "        pos_loss = -torch.mean(torch.log(x_masked_res_pos + 1e-8))\n",
        "        neg_loss = torch.mean(torch.log(x_masked_res_neg + 1e-8))\n",
        "        l1 = torch.mean(x_masked_pos[user_tensors > 0])\n",
        "\n",
        "        combined_loss = self.lambda_pos * pos_loss + self.lambda_neg * neg_loss + self.alpha * l1\n",
        "\n",
        "        return combined_loss, pos_loss, neg_loss, l1\n"
      ],
      "metadata": {
        "id": "he-BrfE_r4x3"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "items_array = np.eye(len(item_encoder.classes_))\n",
        "all_items_tensor = torch.Tensor(items_array).to(device)\n",
        "# Convert test_df to a NumPy array\n",
        "test_array = test_df.values\n",
        "\n",
        "# Create random_sampled_array\n",
        "num_of_rand_users = 200  # Number of users for evaluations\n",
        "random_rows = np.random.choice(test_array.shape[0], num_of_rand_users, replace=False)\n",
        "random_sampled_array = test_array[random_rows]"
      ],
      "metadata": {
        "id": "BFzbDC2fZ1gO"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CupHXPSm_ITg"
      },
      "source": [
        "## HP tune"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
      ],
      "metadata": {
        "id": "8UXzTs492wF6"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install wandb"
      ],
      "metadata": {
        "id": "yIWf4r62WD3h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "import math"
      ],
      "metadata": {
        "id": "RJstNASvWYcE"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lxr_training(trial):\n",
        "    learning_rate = trial.suggest_float('learning_rate', 0.001, 0.01)\n",
        "    alpha = trial.suggest_categorical('alpha', [1])\n",
        "    lambda_neg = trial.suggest_float('lambda_neg', 0, 50)\n",
        "    lambda_pos = trial.suggest_float('lambda_pos', 0, 50)\n",
        "    batch_size = trial.suggest_categorical('batch_size', [32, 64, 16])\n",
        "    explainer_hidden_size = trial.suggest_categorical('explainer_hidden_size', [32, 64, 128])\n",
        "    epochs = 40\n",
        "\n",
        "    path = '/content/drive/MyDrive/Tamar/project/'\n",
        "\n",
        "    # Initialize WandB for logging\n",
        "    wandb.init(\n",
        "        project=f\"LXR_ecommerce\",\n",
        "        name=f\"trial_{trial.number}\",\n",
        "        config={\n",
        "            'learning_rate': learning_rate,\n",
        "            'alpha': alpha,\n",
        "            'lambda_neg': lambda_neg,\n",
        "            'lambda_pos': lambda_pos,\n",
        "            'batch_size': batch_size,\n",
        "            'explainer_hidden_size': explainer_hidden_size,\n",
        "            'architecture': 'LXR_combined',\n",
        "            'activation_function': 'Tanh',\n",
        "            'loss_type': 'logloss',\n",
        "            'optimize_for': 'pos_at_20',\n",
        "            'epochs': epochs\n",
        "        })\n",
        "\n",
        "    loader = torch.utils.data.DataLoader(train_array, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "    num_batches=math.ceil(train_array.shape[0]/batch_size)\n",
        "\n",
        "    print(f\"user_size: {len(user_encoder.classes_)}, item_size: {len(item_encoder.classes_)}\")\n",
        "    print(f\"hidden_size (from best_params): {best_params['hidden_dim']}\")\n",
        "\n",
        "    # Initialize the recommender model\n",
        "    recommender = MLP(user_size=len(user_encoder.classes_),\n",
        "                      item_size=len(item_encoder.classes_),\n",
        "                      hidden_size=best_params['hidden_dim'], device=device).to(device)\n",
        "\n",
        "    # Load the pre-trained weights for the recommender model\n",
        "    recommender.load_state_dict(torch.load(f'{path}/trained_recommenders/trained_best_rec_trial_0_epoch_10_1.0.pth', map_location=device))\n",
        "    recommender.eval()\n",
        "    print(f\"Recommender model initialized on device: {recommender.device}\")\n",
        "\n",
        "    # Initialize the explainer and optimizer\n",
        "    explainer = Explainer(len(user_encoder.classes_), len(item_encoder.classes_), explainer_hidden_size).to(device)\n",
        "    optimizer_comb = torch.optim.Adam(explainer.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Pass the recommender model to the loss function\n",
        "    loss_func = LXR_loss(lambda_pos, lambda_neg, alpha, recommender)\n",
        "\n",
        "    # creating top item dicts\n",
        "    top1_train = {}\n",
        "    top1_test = {}\n",
        "    for i in range(train_array.shape[0]): #iterate over all the rows in train\n",
        "        user_index = train_array[i][-1]\n",
        "        user_tensor = torch.Tensor(train_array[i][:-1]).to(device)\n",
        "        top1_train[user_index] = int(get_user_recommended_item(user_tensor, recommender, **kw_dict))\n",
        "    for i in range(test_array.shape[0]):\n",
        "        user_index = test_array[i][-1]\n",
        "        user_tensor = torch.Tensor(test_array[i][:-1]).to(device)\n",
        "        top1_test[user_index] = int(get_user_recommended_item(user_tensor, recommender, **kw_dict))\n",
        "\n",
        "    print('======================== new run ========================')\n",
        "\n",
        "    # Lists to store POS@20 and NEG@20 metrics\n",
        "    run_pos_at_20 = []\n",
        "    run_neg_at_20 = []\n",
        "    metric_for_monitoring = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"starting epoch {epoch}\")\n",
        "        if epoch % 15 == 0 and epoch > 0:  # Reduce learning rate every 15 epochs\n",
        "            learning_rate *= 0.1\n",
        "            for param_group in optimizer_comb.param_groups:\n",
        "                param_group['lr'] = learning_rate\n",
        "\n",
        "        train_loss = 0\n",
        "        total_pos_loss, total_neg_loss, total_l1_loss = 0, 0, 0\n",
        "        explainer.train()\n",
        "\n",
        "        top1_train = train_df.groupby('user_id')['item_id'].agg(lambda x: x.value_counts().idxmax()).to_dict()\n",
        "\n",
        "        batch_counter = 1\n",
        "        for batch_index, samples in enumerate(loader):\n",
        "            if batch_counter == num_batches:\n",
        "              print (f'final batch - batch num {num_batches}')\n",
        "            if samples.shape[0] < batch_size:\n",
        "              print(f\"Skipping batch due to unexpected batch size: {samples.shape}\")\n",
        "              continue\n",
        "            user_ids = samples[:, 0].to(device).long()\n",
        "            # print(f\"Max user_id: {user_ids.max()}, Min user_id: {user_ids.min()}\")\n",
        "\n",
        "            user_tensors = torch.zeros((user_ids.size(0), len(user_encoder.classes_)), device=device)\n",
        "            user_tensors.scatter_(1, user_ids.unsqueeze(1), 1)\n",
        "            # print(f'user tensors shape: {user_tensors.shape}, user_tensors dtype: {user_tensors.dtype}')\n",
        "\n",
        "            top1_item = np.array([top1_train[int(x)] for x in user_ids.cpu().numpy()])\n",
        "            # print(f\"Top1 items: {top1_item}\")\n",
        "            items_vectors = items_array[top1_item]\n",
        "            items_tensors = torch.tensor(items_vectors, device=device, dtype=torch.float32)\n",
        "            # print(f'item tensors shape: {items_tensors.shape}, item_tensors dtype: {items_tensors.dtype}')\n",
        "\n",
        "            if user_tensors.shape[1] != len(user_encoder.classes_) or items_tensors.shape[1] != len(item_encoder.classes_):\n",
        "              print(f\"Skipping due to shape mismatch: {user_tensors.shape}, {items_tensors.shape}\")\n",
        "              continue\n",
        "\n",
        "            # assert user_tensors.shape == (batch_size, 289), f\"Unexpected user_tensors shape: {user_tensors.shape}\"\n",
        "            # assert items_tensors.shape == (batch_size, 1500), f\"Unexpected items_tensors shape: {items_tensors.shape}\"\n",
        "\n",
        "            optimizer_comb.zero_grad()\n",
        "            # print(user_tensors.shape, items_tensors.shape)\n",
        "            expl_scores = explainer(user_tensors, items_tensors)\n",
        "\n",
        "            # Calculate loss using the recommender model\n",
        "            comb_loss, pos_loss, neg_loss, l1 = loss_func(user_tensors, items_tensors, top1_item, expl_scores)\n",
        "            n = user_tensors.shape[0]\n",
        "            train_loss += comb_loss.item() * n\n",
        "            total_pos_loss += pos_loss.item() * n\n",
        "            total_neg_loss += neg_loss.item() * n\n",
        "            total_l1_loss += l1.item() * n\n",
        "\n",
        "            # Backward pass\n",
        "            # print(f\"Comb Loss: {comb_loss.item()}, Pos Loss: {pos_loss.item()}, Neg Loss: {neg_loss.item()}, L1: {l1.item()}\")\n",
        "            comb_loss.backward()\n",
        "            optimizer_comb.step()\n",
        "\n",
        "            batch_counter += 1\n",
        "\n",
        "        # Evaluate on the test set\n",
        "        explainer.eval()\n",
        "        POS_at_20_lxr, NEG_at_20_lxr = np.zeros(11), np.zeros(11)\n",
        "        for j in range(random_sampled_array.shape[0]):\n",
        "            user_id = random_sampled_array[j][0] #first element of the row is the user id\n",
        "            # user_tensor = torch.Tensor(random_sampled_array[j][:-1]).to(device)\n",
        "            user_tensor = torch.zeros(len(user_encoder.classes_)).to(device)\n",
        "            user_tensor[user_id] = 1.0\n",
        "\n",
        "            top1_test = test_df.groupby('user_id')['item_id'].agg(lambda x: x.value_counts().idxmax()).to_dict()\n",
        "            top1_item_test = top1_test[user_id]\n",
        "            item_vector = torch.Tensor(items_array[top1_item_test]).to(device)\n",
        "\n",
        "            pos_neg_res = calculate_pos_neg_k(user_tensor, top1_item_test, item_vector, num_of_bins=10, explainer=explainer, k=20)\n",
        "            POS_at_20_lxr += pos_neg_res[0]\n",
        "            NEG_at_20_lxr += pos_neg_res[1]\n",
        "\n",
        "        last_pos_at_20 = np.mean(POS_at_20_lxr) / random_sampled_array.shape[0]\n",
        "        last_neg_at_20 = np.mean(NEG_at_20_lxr) / random_sampled_array.shape[0]\n",
        "\n",
        "        run_pos_at_20.append(last_pos_at_20)\n",
        "        run_neg_at_20.append(last_neg_at_20)\n",
        "        metric_for_monitoring.append(last_pos_at_20)\n",
        "\n",
        "        wandb.log({\"val/pos_at_20\": last_pos_at_20, \"val/neg_at_20\": last_neg_at_20})\n",
        "\n",
        "        # Early stopping\n",
        "        if epoch >= 5 and all(run_pos_at_20[-i-1] < run_pos_at_20[-i] for i in range(3)) and all(run_neg_at_20[-i-1] > run_neg_at_20[-i] for i in range(3)):\n",
        "            print(f'Early stop at epoch {epoch}')\n",
        "            break\n",
        "\n",
        "    # Return best metric value for optimization\n",
        "    torch.save(explainer.state_dict(), f'{path}/trained_exp/trained_best_exp_trial_{trial.number}_epoch_{epoch + 1}_{np.min(metric_for_monitoring)}.pth')\n",
        "    return np.min(metric_for_monitoring)\n"
      ],
      "metadata": {
        "id": "wwf1tt_csM4_"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logger = logging.getLogger()\n",
        "logger.setLevel(logging.INFO)\n",
        "logger.addHandler(logging.FileHandler(f\"explainer_training.log\", mode=\"w\"))\n",
        "\n",
        "# Create Optuna study and optimize\n",
        "optuna.logging.enable_propagation()\n",
        "optuna.logging.disable_default_handler()\n",
        "\n",
        "study = optuna.create_study(direction='minimize')\n",
        "logger.info(\"Start optimization.\")\n",
        "study.optimize(lxr_training, n_trials=20)\n",
        "\n",
        "# Print the best hyperparameters\n",
        "print(\"Best hyperparameters: {}\".format(study.best_params))\n",
        "print(\"Best metric value: {}\".format(study.best_value))"
      ],
      "metadata": {
        "id": "lFXWOccHsM90"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VP1-WaWFAfp4"
      },
      "source": [
        "## finetune trained original LXR"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### handle pre trained model"
      ],
      "metadata": {
        "id": "uCfCpZ73M85B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#same as the original code\n",
        "class pre_trained_Explainer(nn.Module):\n",
        "    def __init__(self, user_size, item_size, hidden_size):\n",
        "        super(pre_trained_Explainer, self).__init__()\n",
        "\n",
        "        self.users_fc = nn.Linear(in_features = user_size, out_features=hidden_size).to(device)\n",
        "        self.items_fc = nn.Linear(in_features = item_size, out_features=hidden_size).to(device)\n",
        "        self.bottleneck = nn.Sequential(\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(in_features = hidden_size*2, out_features=hidden_size).to(device),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(in_features = hidden_size, out_features=user_size).to(device),\n",
        "            nn.Sigmoid()\n",
        "        ).to(device)\n",
        "\n",
        "\n",
        "    def forward(self, user_tensor, item_tensor):\n",
        "        user_output = self.users_fc(user_tensor.float())\n",
        "        item_output = self.items_fc(item_tensor.float())\n",
        "        combined_output = torch.cat((user_output, item_output), dim=-1)\n",
        "        expl_scores = self.bottleneck(combined_output).to(device)\n",
        "        return expl_scores"
      ],
      "metadata": {
        "id": "52pvf257TqQz"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_size = 128\n",
        "embedding_dim = 64\n",
        "lxr = pre_trained_Explainer(user_size=hidden_size, item_size=hidden_size, hidden_size=hidden_size) #create model\n",
        "model_dict = lxr.state_dict() #get the current model's state dict - with the current dimentions"
      ],
      "metadata": {
        "id": "4pY5TdNzTx32"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load pre trained\n",
        "checkpoints_path = '/content/drive/MyDrive/Tamar/project/original paper LXR/checkpoints/'\n",
        "specific_lxr = 'LXR_ML1M_MLP_12_39_64_11.59908096547193_0.1414854294885049.pt'\n",
        "state_dict = torch.load(f'{checkpoints_path}{specific_lxr}', map_location=torch.device('cpu'))\n",
        "\n",
        "#ignore fc layers that mismatch the current dataset's dimentions\n",
        "pretrained_dict = {k: v for k, v in state_dict.items() if k in model_dict and v.size() == model_dict[k].size()}\n",
        "model_dict.update(pretrained_dict)\n",
        "\n",
        "lxr.load_state_dict(model_dict) #load the state dict of the pretrained model minus the fc layers (due to dimention mismatch)"
      ],
      "metadata": {
        "id": "TCsJweSbKArN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ea5cfbd-10bf-4ac1-8d6c-e03dce2558e7"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-67-5be0c7021993>:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(f'{checkpoints_path}{specific_lxr}', map_location=torch.device('cpu'))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_dict.keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ggvpvRKBRsR",
        "outputId": "fb79be9a-e030-421e-a0a0-de2e7f5a94fd"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "odict_keys(['users_fc.weight', 'users_fc.bias', 'items_fc.weight', 'items_fc.bias', 'bottleneck.1.weight', 'bottleneck.1.bias', 'bottleneck.3.weight', 'bottleneck.3.bias'])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#freeze all layers except fc\n",
        "for name, param in lxr.named_parameters():\n",
        "    if name not in ['users_fc.weight', 'users_fc.bias', 'items_fc.weight', 'items_fc.bias']:\n",
        "        param.requires_grad = False"
      ],
      "metadata": {
        "id": "FbdYcLSNBHvi"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### handle data"
      ],
      "metadata": {
        "id": "qFkIUETntRvz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_ids = torch.tensor(filtered_user_item_df['user_id'].values, dtype=torch.long)\n",
        "item_ids = torch.tensor(filtered_user_item_df['item_id'].values, dtype=torch.long)\n",
        "interactions = torch.tensor(filtered_user_item_df['interaction'].values, dtype=torch.float)"
      ],
      "metadata": {
        "id": "8rj0mxAPtR7b"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class AmazonInteractionDataset(Dataset):\n",
        "    def __init__(self, user_ids, item_ids, interactions):\n",
        "        self.user_ids = user_ids\n",
        "        self.item_ids = item_ids\n",
        "        self.interactions = interactions\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.user_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.user_ids[idx], self.item_ids[idx], self.interactions[idx]\n",
        "\n",
        "# Instantiate the dataset\n",
        "dataset = AmazonInteractionDataset(user_ids, item_ids, interactions)\n",
        "\n",
        "# Create DataLoader\n",
        "train_loader = DataLoader(dataset, batch_size=32, shuffle=True)"
      ],
      "metadata": {
        "id": "eizk6XQwtR-I"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "d8qLwAoZtSA6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### architecture of explainer added on top"
      ],
      "metadata": {
        "id": "VL14pFV5NDqs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class New_explainer(nn.Module):\n",
        "    def __init__(self, user_size, item_size, hidden_size, embedding_dim, lxr_model):\n",
        "        super(New_explainer, self).__init__()\n",
        "\n",
        "        self.user_size = user_size\n",
        "        self.item_size = item_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # Embedding layers for users and items\n",
        "        self.user_embedding = nn.Embedding(user_size, embedding_dim).to(device)\n",
        "        self.item_embedding = nn.Embedding(item_size, embedding_dim).to(device)\n",
        "\n",
        "        # LXR model\n",
        "        self.lxr_model = lxr_model\n",
        "\n",
        "        # Additional layers to refine the explanation scores\n",
        "        self.refinement_layer_user = nn.Linear(embedding_dim, embedding_dim).to(device)\n",
        "        self.refinement_layer_item = nn.Linear(embedding_dim, embedding_dim).to(device)\n",
        "\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, user_ids, item_ids):\n",
        "        # Convert IDs to embeddings\n",
        "        user_tensor = self.user_embedding(user_ids)\n",
        "        item_tensor = self.item_embedding(item_ids)\n",
        "\n",
        "        # Get LXR's explanation scores\n",
        "        with torch.no_grad():\n",
        "            lxr_scores = self.lxr_model(user_tensor, item_tensor)\n",
        "\n",
        "        # Refinement through additional layers\n",
        "        expl_scores_user = self.sigmoid(self.refinement_layer_user(user_tensor))\n",
        "        expl_scores_item = self.sigmoid(self.refinement_layer_item(item_tensor))\n",
        "\n",
        "        # Apply the explanation scores to the original inputs\n",
        "        user_tensor_adjusted = user_tensor * expl_scores_user\n",
        "        item_tensor_adjusted = item_tensor * expl_scores_item\n",
        "\n",
        "        return user_tensor_adjusted, item_tensor_adjusted\n"
      ],
      "metadata": {
        "id": "q33DR9MBCiKx"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the new explainer\n",
        "new_explainer = New_explainer(n_users, n_items, 128, 128, lxr)\n",
        "\n",
        "# Define an optimizer for the new explainer\n",
        "optimizer = torch.optim.Adam(new_explainer.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "BnOgTTWzI7Md"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss function (using the unique LXR loss function concept)\n",
        "def lxr_loss_function(f_x, f_xm):\n",
        "    return torch.mean((f_x - f_xm) ** 2)  # L2 loss for minimizing the distance between recommender's outputs when given original X and when given X*m"
      ],
      "metadata": {
        "id": "c-O10RrlJOtU"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(num_epochs, optimizer, train_loader, test_loader, lxr, new_explainer, recommender_model):\n",
        "    best_val_loss = float('inf')  # Initialize best validation loss\n",
        "    path = '/content/drive/MyDrive/Tamar/project/trained_exp'\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "        # Training phase\n",
        "        new_explainer.train()  # Set new explainer to training mode\n",
        "        running_train_loss = 0.0\n",
        "\n",
        "        for batch in train_loader:\n",
        "            user_tensor, item_tensor, interaction_tensor = batch\n",
        "\n",
        "            optimizer.zero_grad()  # Reset gradients\n",
        "\n",
        "            # Get the adjusted tensors from the explainer\n",
        "            user_tensor_adjusted, item_tensor_adjusted = new_explainer(user_tensor, item_tensor)\n",
        "\n",
        "            # Compute f(x) using the original input (without applying explanation scores)\n",
        "            f_x = recommender_model(user_tensor, item_tensor)\n",
        "\n",
        "            # Compute f(x*m) using the adjusted tensors from the new explainer\n",
        "            f_xm_new = recommender_model(user_tensor_adjusted, item_tensor_adjusted)\n",
        "\n",
        "            # Compute the loss between f(x) and f(x*m)\n",
        "            loss = lxr_loss_function(f_x, f_xm_new)\n",
        "            running_train_loss += loss.item()\n",
        "\n",
        "            # Backward pass and optimization\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # Calculate average training loss for the epoch\n",
        "        avg_train_loss = running_train_loss / len(train_loader)\n",
        "\n",
        "        # Validation phase\n",
        "        new_explainer.eval()  # Set new explainer to evaluation mode\n",
        "        running_val_loss = 0.0\n",
        "\n",
        "        with torch.no_grad():  # Disable gradient computation for validation\n",
        "            for batch in test_loader:\n",
        "                user_tensor, item_tensor, interaction_tensor = batch\n",
        "\n",
        "                #get results of explainer (m), recommender and recommender for x*m\n",
        "                user_tensor_adjusted, item_tensor_adjusted = new_explainer(user_tensor, item_tensor)\n",
        "                f_x = recommender_model(user_tensor, item_tensor)\n",
        "                f_xm_new = recommender_model(user_tensor_adjusted, item_tensor_adjusted)\n",
        "\n",
        "                loss = lxr_loss_function(f_x, f_xm_new)\n",
        "                running_val_loss += loss.item()\n",
        "\n",
        "        # Calculate average validation loss for the epoch\n",
        "        avg_val_loss = running_val_loss / len(test_loader)\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}')\n",
        "\n",
        "        # Check if the current validation loss is the best we've seen so far\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            # Save the model state\n",
        "            torch.save(new_explainer.state_dict(), f'{path}/exp_epoch_{epoch}_loss_{best_val_loss:.4f}.pth')\n",
        "            print(f'Best model in epoch {epoch+1} saved with validation loss: {best_val_loss:.4f}')\n"
      ],
      "metadata": {
        "id": "4Achpkwn4hYF"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### finetune and train top explainer"
      ],
      "metadata": {
        "id": "5THkl8GQNLC_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the recommender model\n",
        "recommender = MLP(user_size=len(user_encoder.classes_),\n",
        "                  item_size=len(item_encoder.classes_),\n",
        "                  hidden_size=best_params['hidden_dim'], device=device).to(device)\n",
        "\n",
        "# Load the pre-trained weights for the recommender model\n",
        "path = '/content/drive/MyDrive/Tamar/project/'\n",
        "recommender.load_state_dict(torch.load(f'{path}/trained_recommenders/trained_best_rec_trial_0_epoch_10_1.0.pth', map_location=device))\n",
        "recommender.eval()\n",
        "print(f\"Recommender model initialized on device: {recommender.device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ikTF8S2pVMIo",
        "outputId": "f00cc01d-4103-4478-8e90-34f30dece663"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Recommender model initialized on device: cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-75-ef6dede3d4d3>:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  recommender.load_state_dict(torch.load(f'{path}/trained_recommenders/trained_best_rec_trial_0_epoch_10_1.0.pth', map_location=device))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train(50, optimizer, train_loader, test_loader, lxr, new_explainer, recommender)"
      ],
      "metadata": {
        "id": "3C0q7In9NR7K"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wvheN2MZDE-n"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}